{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrhallonline/NLP-Workshop/blob/main/Module_2_Workshop_Setting_Up_Natural_Language_Toolkit_(NLTK)_V3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.0 Setting Up (?? minutes)\n",
        "\n",
        "Click below to install the libraries:\n",
        "1. NLTK\n",
        "2.   [Matplotlib](https://matplotlib.org/) library: Library for creating data visualizations.\n",
        "3.   [Gensim](https://pypi.org/project/gensim/) library: Natural language processing tool\n",
        "4.   [PyPDF2](https://pypdf2.readthedocs.io/en/3.0.0/index.html#) library: NLTK normally works with text files, PyPDF2 will allow you to read, write, convert to text, and merge pdf files\n",
        "5. Numpy\n",
        "6. Pandas\n",
        "\n",
        "\n",
        "\n",
        "###Google Colab is similar in usage to software like RStudio allowing you to run chunks or cells at a time. Click in this next code cell. An output should appear under it listing the version of python used in this Colab notebook."
      ],
      "metadata": {
        "id": "gE_RM-qcfLwU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "id": "KSIYEGtMy7Vj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18fd3333-9568-48ac-b34b-665f38122357"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73VqKds77kv5"
      },
      "source": [
        "## 2.1 Installing NLTK supporting dependencies and libraries.\n",
        "Once we know that python is running you can click the following code cell to automatically download and install NLTK and the dependencies that we will be using throughout this workshop. Don't worry if you have already installed it prior to this. If installed you will see the output mention \"Requirement already satisfied:\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7Vu9Lmbv8gKG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41ef97eb-ca3a-41a5-daec-8ec76911bfe3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.3.0)\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n",
        "!pip install matplotlib\n",
        "!pip install gensim\n",
        "!pip install PyPDF2\n",
        "!pip install numpy\n",
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.2 Importing NLTK and popular corpora\n",
        "\n",
        "NLTK has access to a wide range of text and audio corpora that can be easily viewed and analyzed if you are ever in need of data to mess around with. We wont spend time with it today but clicking on the following code cell will import nltk and download the most widely used corpora. This \"import nltk\" code is needed to be run at least once but can be placed at the head of any code cell just to be certain."
      ],
      "metadata": {
        "id": "LfASyDjBx-fT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('popular')"
      ],
      "metadata": {
        "id": "F6LNgYKOx4wl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62bc53a6-1503-4e1a-c6bf-f9e28628b197"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JD_f-gjP4cmC"
      },
      "source": [
        "## 2.3  Connecting/Mounting your Google drive to be accessible in Google Colab\n",
        "\n",
        "Click the following cell to connect the current Google Colab notebook to you Google Drive to save and access data. This connection is temporary and you will need to connect again after some passage of time without usage. If after some time you run into errors stating files can't be found, clicking this again would make sure the connection is still live.\n",
        "\n",
        "#### When clicked, the output will let you know if the drive is already mounted, if not it will ask for your authorization to connect to Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oluf846u38Ey",
        "outputId": "7c95f3c6-6a89-4d78-8836-2a8323cace82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VB6YY4rns_7b"
      },
      "source": [
        "# 2.4 Importing the Google Sheets file containing our raw transcript corpus to be used in Google Colab.\n",
        "Clicking the next code cell will automatically download the csv file containing our data corpus and save it locally as an Excel file in Google Drive so it can be accessed by this Colab document. You can use this same code to download any file that you have sharing access with, simply change the file_id with the new one in line 7 and changing the filename and/or filetype in line 16.\n",
        "\n",
        "\n",
        "#### The file will show in your folder several seconds after running the code cell\n",
        "\n",
        "#### This is also temporary unless saved directly in your Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "K6M3iUIqz3jA"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "# This is the full shared Drive link,\n",
        "# https://docs.google.com/spreadsheets/d/1iJ4SG-QXfY4zw5K9B7Ununv3rb3iBj8S/edit?usp=drive_link&ouid=106477043869312333876&rtpof=true&sd=true\n",
        "\n",
        "# get the file ID from the shareable link and paste below\n",
        "file_id = \"1iJ4SG-QXfY4zw5K9B7Ununv3rb3iBj8S\"\n",
        "\n",
        "# construct the download URL\n",
        "download_url = f\"https://docs.google.com/uc?export=download&id={file_id}\"\n",
        "\n",
        "# send a GET request to the download URL and save the response content\n",
        "response = requests.get(download_url)\n",
        "\n",
        "# The next line names the file after download. If you change it here, you will also need to change in the subsequent fields.\n",
        "with open(\"uncertaintyText.xlsx\", \"wb\") as f:\n",
        "    f.write(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FhS-iBrvsYC"
      },
      "source": [
        "# 2.5 Working with csv Files\n",
        "\n",
        "Now that the csv file is locally accessible, clicking this code cell will open the Excel file called uncertaintyText.xlsx found in the content folder and copy each row of text found in the column titled \"transcript\" and write that data to a text file called \"raw_uncertaintyText.txt and save it in your Google Drive instead of the temporary cloud folder.\n",
        "###Importantly, the whole text corpus is also saved in a variable called raw_uncertaintyText, which is what we will be working with as our raw unprocessed \"uncertainty\" data as we move forward. The text file is only needed if we need to do this process again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4Vqwl-fQv3lt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e5a7a02-db7a-4e4a-ee53-42a814b76947"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text saved to raw_uncertaintyText.txt\n",
            "It's table 6, right? I think so. Always go with basic assumptions. Unless it's about someone in that case, don't. Eric. Yes. I've been sitting with you. Don't like it because we're right in front of Ms. Fletcher. But it's okay. You can live with it. \n"
          ]
        }
      ],
      "source": [
        "!pip install pandas openpyxl nltk\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Specify the path to the Excel file\n",
        "excel_file_path = '/content/uncertaintyText.xlsx'\n",
        "\n",
        "# Specify the column name you want to tokenize\n",
        "column_name = 'transcript'\n",
        "\n",
        "# Read the Excel file and extract the specified column\n",
        "data = pd.read_excel(excel_file_path, engine='openpyxl')\n",
        "text_column = data[column_name]\n",
        "\n",
        "\n",
        "# Convert each item in the column to a string and then join them\n",
        "raw_uncertaintyText = ' '.join(map(str, text_column))\n",
        "\n",
        "\n",
        "# Save the string to a text file in your Google Drive\n",
        "with open('/content/drive/MyDrive/raw_uncertaintyText.txt', 'w') as file:\n",
        "  file.write(raw_uncertaintyText)\n",
        "\n",
        "print(\"Text saved to raw_uncertaintyText.txt\")\n",
        "print(raw_uncertaintyText[0:250])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxXvUGHZ2b7g"
      },
      "source": [
        "# 2.6 Working directly from Text Files\n",
        "Text files can be used directly and don't need to be converted in order to be imported into NLTK. Many other file types, other than raw text and downloaded corpora, will need to be first initially converted to text files in order to be used in our NLTK data flow. Use the following code cell if you want to load your data directly from a text file without need to convert from csv or pdf for example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "lZ7Ug5kv2acW"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "\n",
        "# load data from existing text file\n",
        "filename = '/content/drive/MyDrive/raw_uncertaintyText.txt'\n",
        "uncertaintyText = open(filename, 'rt', encoding='utf-8', errors='replace')\n",
        "\n",
        "raw_uncertaintyText = uncertaintyText.read()\n",
        "uncertaintyText.close()\n",
        "\n",
        "# Word Tokenization\n",
        "uncertainty_wordTokens = nltk.word_tokenize(raw_uncertaintyText)\n",
        "\n",
        "# Creating a Text object from the tokens\n",
        "uncertainty_wordTextObjects = nltk.Text(uncertainty_wordTokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.7 Basic information about the data corpus\n",
        "\n",
        "From the original csv file located in my Google Drive, you have now created 5 documents:\n",
        "\n",
        "1. Excel File = uncertaintyText.xlsx\n",
        "2. Text File = raw_uncertaintyText.txt\n",
        "3. Uncertainty text variable = raw_uncertaintyText\n",
        "4. Uncertainty text as word tokens = uncertainty_wordTokens\n",
        "\n",
        "If you run the code cell below you can notice some differences between the documents. The differences are unimportant but it is important to know that you can always figure out what type of data corpus you are dealing with by running these print checks. It is also extremely important to also note the importance of keeping your documents categorized, lest they get out of control.\n",
        "* We will look at the utility of numbers 3 and 4 in the next module using some of the features of natural language processing using NLTK."
      ],
      "metadata": {
        "id": "rzYxRbGsWVWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number 3 is a: \",type(raw_uncertaintyText))\n",
        "print(\"Number 4 is a: \",type(uncertainty_wordTokens))\n",
        "print(\"Number 5 is a: \",type(uncertainty_wordTextObjects))\n",
        "\n",
        "print(\"Number of characters in number 3 is: \",len(raw_uncertaintyText))\n",
        "print(\"Number of characters in number 4 is: \",len(uncertainty_wordTokens))\n",
        "print(\"Number of characters in number 5 is: \",len(uncertainty_wordTextObjects))\n",
        "\n",
        "print(\"Here are the first 100 characters in number 3: \",raw_uncertaintyText[0:100])\n",
        "print(\"Here are the first 100 characters in number 4: \",uncertainty_wordTokens[0:100])\n",
        "print(\"Here are the first 100 characters in number 5: \",uncertainty_wordTextObjects[0:100])"
      ],
      "metadata": {
        "id": "TmQmWaWn2Lzj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10fb3de2-2bd7-4aad-b06c-0c86edb77e0c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number 3 is a:  <class 'str'>\n",
            "Number 4 is a:  <class 'list'>\n",
            "Number 5 is a:  <class 'nltk.text.Text'>\n",
            "Number of characters in number 3 is:  66207\n",
            "Number of characters in number 4 is:  16771\n",
            "Number of characters in number 5 is:  16771\n",
            "Here are the first 100 characters in number 3:  It's table 6, right? I think so. Always go with basic assumptions. Unless it's about someone in that\n",
            "Here are the first 100 characters in number 4:  ['It', \"'s\", 'table', '6', ',', 'right', '?', 'I', 'think', 'so', '.', 'Always', 'go', 'with', 'basic', 'assumptions', '.', 'Unless', 'it', \"'s\", 'about', 'someone', 'in', 'that', 'case', ',', 'do', \"n't\", '.', 'Eric', '.', 'Yes', '.', 'I', \"'ve\", 'been', 'sitting', 'with', 'you', '.', 'Do', \"n't\", 'like', 'it', 'because', 'we', \"'re\", 'right', 'in', 'front', 'of', 'Ms.', 'Fletcher', '.', 'But', 'it', \"'s\", 'okay', '.', 'You', 'can', 'live', 'with', 'it', '.', 'Did', 'you', '...', 'did', 'you', 'do', 'the', 'search', '?', 'Yes', '.', 'It', \"'s\", 'for', 'next', 'class', '.', 'I', 'know', ',', 'I', 'found', 'out', '.', 'I', \"'m\", 'so', 'mad', 'because', 'we', \"'re\", 'not', 'going', 'to', 'be']\n",
            "Here are the first 100 characters in number 5:  ['It', \"'s\", 'table', '6', ',', 'right', '?', 'I', 'think', 'so', '.', 'Always', 'go', 'with', 'basic', 'assumptions', '.', 'Unless', 'it', \"'s\", 'about', 'someone', 'in', 'that', 'case', ',', 'do', \"n't\", '.', 'Eric', '.', 'Yes', '.', 'I', \"'ve\", 'been', 'sitting', 'with', 'you', '.', 'Do', \"n't\", 'like', 'it', 'because', 'we', \"'re\", 'right', 'in', 'front', 'of', 'Ms.', 'Fletcher', '.', 'But', 'it', \"'s\", 'okay', '.', 'You', 'can', 'live', 'with', 'it', '.', 'Did', 'you', '...', 'did', 'you', 'do', 'the', 'search', '?', 'Yes', '.', 'It', \"'s\", 'for', 'next', 'class', '.', 'I', 'know', ',', 'I', 'found', 'out', '.', 'I', \"'m\", 'so', 'mad', 'because', 'we', \"'re\", 'not', 'going', 'to', 'be']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.8 Messing Around With Concordance and Text Objects\n",
        "##Anonymizing data corpus\n"
      ],
      "metadata": {
        "id": "l_XqYn45ViUB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.81 Searching for names"
      ],
      "metadata": {
        "id": "ksmfDUm3qyb3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.text import Text\n",
        "\n",
        "\n",
        "uncertainty_wordTextObjects.concordance(\"felix\", lines = 25, width=200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJWgrmZ6Yk1U",
        "outputId": "bd7bddba-6339-486a-c1d0-a5721ff063a3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Displaying 3 of 3 matches:\n",
            "u 're going to want to use that word a lot . That 's too slow to hit this . All right . Thanks , Felix . I guess Nate , what 'd you miss ? So the same trick that I had all the answers fitting out rig\n",
            "t right next to each other , I 'm going to put both of them in the equation itself . Anyway , as Felix found , the answer found is in here , 4.62 . But this can also be attributed to , since we know \n",
            "e exactly ? I mean , it 's one radian , but what else is it exactly ? Yeah . Yeah , what is it , Felix ? It 's 57.29577 . Is that exact ? Still getting me the decimal point . I was just looking for l\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.82 Replacing names"
      ],
      "metadata": {
        "id": "NgSD4Tfwhfzk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "replace_dict = {\n",
        "    'Eric': 'Steven',\n",
        "    'Felix': 'Jason'\n",
        "}\n",
        "\n",
        "# Replace words in your original token list\n",
        "names_uncertainty_wordTokens = [replace_dict.get(word, word) for word in uncertainty_wordTokens]\n",
        "\n",
        "# You can directly create a Text object from the tokens for concordance\n",
        "text_obj = Text(names_uncertainty_wordTokens)\n",
        "\n",
        "# Check replacements with concordance\n",
        "text_obj.concordance('felix')\n",
        "text_obj.concordance('eric')\n",
        "text_obj.concordance('Steven')\n",
        "text_obj.concordance('Jason')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56XRmo6CmT0z",
        "outputId": "786bf6e9-0e5e-4dad-821b-d4f769d81144"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no matches\n",
            "no matches\n",
            "Displaying 4 of 4 matches:\n",
            "out someone in that case , do n't . Steven . Yes . I 've been sitting with you\n",
            "w what to do with it . According to Steven , there 's no solutions . Yeah , th\n",
            " supposed to solve for X . What did Steven say ? There 's no solutions . Steve\n",
            "teven say ? There 's no solutions . Steven 's wrong . But look at the graph . \n",
            "Displaying 3 of 3 matches:\n",
            "w to hit this . All right . Thanks , Jason . I guess Nate , what 'd you miss ? \n",
            "in the equation itself . Anyway , as Jason found , the answer found is in here \n",
            "exactly ? Yeah . Yeah , what is it , Jason ? It 's 57.29577 . Is that exact ? S\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}