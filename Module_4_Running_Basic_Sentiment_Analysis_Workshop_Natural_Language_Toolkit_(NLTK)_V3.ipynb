{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrhallonline/NLP-Workshop/blob/main/Module_4_Running_Basic_Sentiment_Analysis_Workshop_Natural_Language_Toolkit_(NLTK)_V3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.0 Running Basic Sentiment Analysis on Our Uncertainty Corpus\n"
      ],
      "metadata": {
        "id": "wwL3SA-XgE8O"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwN-_b4jJkGO"
      },
      "source": [
        "## 4.1 Reconnecting to Google Drive and basic processing\n",
        "Once again since we are on to a new Colab notebook. Mounting Google drive so we can access our \"raw_uncertaintyText.txt\" file to once again extract our features from. We are also installing \"textblob\" and downloading the \"vader_lexicon\" both of which we will need to run our basicsentiment analysis. You will notice that at this point we have just combined them all into one code cell since we have already worked with them before.  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import requests\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "# This is the full shared Drive link, the file ID starts at \"1i\" and ends at \"8S\"\n",
        "# https://docs.google.com/spreadsheets/d/1iJ4SG-QXfY4zw5K9B7Ununv3rb3iBj8S/edit?usp=drive_link&ouid=106477043869312333876&rtpof=true&sd=true\n",
        "\n",
        "# the file ID from the shareable link is pasted below in orange.\n",
        "file_id = \"1iJ4SG-QXfY4zw5K9B7Ununv3rb3iBj8S\"\n",
        "\n",
        "# construct the download URL, you would not need to change anything here.\n",
        "download_url = f\"https://docs.google.com/uc?export=download&id={file_id}\"\n",
        "\n",
        "# send a GET request to the download URL and save the response content\n",
        "response = requests.get(download_url)\n",
        "\n",
        "# The next line names the file after download. If you change it here, you will also need to change in the subsequent fields.\n",
        "# If you click on the folder icon in Colab you should see a file now appear called \"uncertaintyText.xlsx\"\n",
        "# These names can be changed to suit you own data\n",
        "with open(\"uncertaintyText.xlsx\", \"wb\") as f:\n",
        "    f.write(response.content)\n",
        "\n",
        "\n",
        "# Specify the path to the Excel file this where it was placed in 2.4 so that is the file and path you want to open\n",
        "excel_file_path = '/content/uncertaintyText.xlsx'\n",
        "\n",
        "# Specify the column name you want to pull the data corpus from\n",
        "column_name = 'transcript'\n",
        "\n",
        "# Read the Excel file and extract the specified column\n",
        "data = pd.read_excel(excel_file_path, engine='openpyxl')\n",
        "text_column = data[column_name]\n",
        "\n",
        "\n",
        "# Convert each item in the column to a string and then join them together to be saved as a text file containing all data in the transcript column.\n",
        "raw_uncertaintyText = ' '.join(map(str, text_column))\n",
        "\n",
        "\n",
        "# Save the string to a text file in your Google Drive\n",
        "with open('/content/raw_uncertaintyText.txt', 'w') as file:\n",
        "  file.write(raw_uncertaintyText)\n",
        "# load data from existing text file\n",
        "## this following path can be changed to access other text files that you may like to work with.\n",
        "filename = '/content/raw_uncertaintyText.txt'\n",
        "uncertaintyText = open(filename, 'rt', encoding='utf-8', errors='replace')\n",
        "\n",
        "# Extracting Different Features for analysis\n",
        "\n",
        "# The raw unchanged data from the text file now exists as a variable called \"raw_uncertaintyText\"\n",
        "raw_uncertaintyText = uncertaintyText.read()\n",
        "uncertaintyText.close()\n",
        "\n",
        "# 1. This line converts our raw text into word tokens\n",
        "# Regular expression tokenizing Gaps =False\n",
        "pattern = r'\\s+'\n",
        "uncertainty_wordTokens = nltk.regexp_tokenize(raw_uncertaintyText, pattern, gaps=True)\n",
        "\n",
        "# 2. This line converts our raw text into sentence tokens\n",
        "uncertainty_sentTokens = nltk.sent_tokenize(raw_uncertaintyText)\n",
        "\n",
        "# 3. This line converts our word tokens into NLTK text objects from the tokens\n",
        "uncertainty_wordTextObjects = nltk.Text(uncertainty_wordTokens)\n",
        "\n",
        "print(uncertainty_wordTokens)\n",
        "\n",
        "print(\"raw_uncertaintyText is a: \",type(raw_uncertaintyText))\n",
        "print(\"uncertainty_wordTokens is a: \",type(uncertainty_wordTokens))\n",
        "print(\"uncertainty_sentTokens is a: \",type(uncertainty_sentTokens))\n",
        "print(\"uncertainty_wordTextObjects is a: \",type(uncertainty_wordTextObjects))"
      ],
      "metadata": {
        "id": "F4ONYirwzKm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VJ8NdutmkU2"
      },
      "source": [
        "# 4.2 Sentiment Analysis\n",
        "\n",
        "The VADER sentiment intensity analyzer that we installed above is used to calculate a compound sentiment score for each sentence that contains one of the predefined \"uncertainty\" words, like \"maybe,\" \"probably,\" \"might,\" etc. These scores range from -1 (most negative) to +1 (most positive), with scores around 0 being neutral.\n",
        "\n",
        "The results could potentially reveal patterns, such as whether sentences containing words of uncertainty tend to have a generally positive, negative, or neutral sentiment. This could offer valuable insights into the emotional landscape of the classroom, particularly regarding how students express and navigate uncertainty in the learning process."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4.3  Sentiment Analysis of Uncertainty Words in Classroom Transcripts\n",
        "This code performs sentence-level sentiment analysis on our uncertainty transcript with a focus on words that indicate uncertainty. You can add or subtract words from the \"uncertainty_words\" dictionary in line 6. NLTK will keep track of frequency counts of each word and then analyze the sentiment intensities of the SENTENCE where the word is found. Giving the overall average sentiment for the sentences. The idea is to understand not just how often these words occur, but also the general emotional tone of the sentences in which they appear."
      ],
      "metadata": {
        "id": "7tPJElZEXR8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('vader_lexicon')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from textblob import TextBlob\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "\n",
        "# Create a list of words representing uncertainty\n",
        "uncertainty_words = [\"maybe\", \"probably\", \"might\", \"perhaps\", \"unsure\", \"uncertain\", \"I think\", \"I guess\"]\n",
        "\n",
        "# text from the high school math class transcript\n",
        "\n",
        "# Sentence Tokenization\n",
        "cleaned_sentences = sent_tokenize(raw_uncertaintyText)\n",
        "\n",
        "# Initialize VADER sentiment intensity analyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Create a dictionary to hold frequency count and sentiment scores for each uncertainty word\n",
        "uncertainty_dict = {}\n",
        "\n",
        "# Loop through each sentence in the cleaned transcript to find instances of uncertainty\n",
        "for sentence in cleaned_sentences:\n",
        "    tb = TextBlob(sentence)\n",
        "    for word in uncertainty_words:\n",
        "        if word.lower() in sentence.lower():\n",
        "            if word not in uncertainty_dict:\n",
        "                uncertainty_dict[word] = {'count': 0, 'sentiment': []}\n",
        "            uncertainty_dict[word]['count'] += 1\n",
        "            uncertainty_dict[word]['sentiment'].append(sia.polarity_scores(sentence)['compound'])\n",
        "\n",
        "# Calculate average sentiment for each word of uncertainty\n",
        "for word, data in uncertainty_dict.items():\n",
        "    avg_sentiment = sum(data['sentiment']) / len(data['sentiment']) if data['sentiment'] else 0\n",
        "    print(f\"Word: {word}, Frequency: {data['count']}, Average Sentiment: {avg_sentiment}\")\n"
      ],
      "metadata": {
        "id": "e3tce4K_1dUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.4 Contextual Analysis of Sentences Sentiment Surrounding Uncertainty Words:\n",
        "\n",
        "In this code snippet, the focus is on examining the sentences that appear immediately before and after a sentence containing the word \"maybe,\" which serves as an example of an \"uncertainty word.\"\n",
        "In this case, when \"maybe\" is found in a sentence, the code adds the sentence immediately preceding it to before_uncertain bucket and the one immediately following it to after_uncertain bucket. Finally, the sentences stored in before_uncertain and after_uncertain are printed for researcher analysis. These can also be saved to text or csv files if needed for later usage.\n",
        "\n",
        "This approach could be valuable for understanding the context in which uncertainty is expressed. By looking at the sentences before and after, we can gain insights into what typically precedes or follows an expression of uncertainty, and how that might inform our understanding of the emotional or instructional dynamics of the classroom."
      ],
      "metadata": {
        "id": "GHwh2Z4Q3hOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "before_uncertain = []\n",
        "after_uncertain = []\n",
        "for i, sentence in enumerate(cleaned_sentences):\n",
        "    if \"maybe\" in sentence:  # Replace with any uncertainty word\n",
        "        if i > 0:\n",
        "            before_uncertain.append(cleaned_sentences[i-1])\n",
        "        if i < len(cleaned_sentences) - 1:\n",
        "            after_uncertain.append(cleaned_sentences[i+1])\n",
        "\n",
        "# Now analyze sentiment for these sentences.\n",
        "print(before_uncertain)\n",
        "print(after_uncertain)"
      ],
      "metadata": {
        "id": "79x6B4py3lQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question:\n",
        "\n",
        "1. What are ways that the we see students interact before moments of uncertainty?\n",
        "\n",
        "2. What are ways that the we see students interact after moments of uncertainty?\n",
        "\n"
      ],
      "metadata": {
        "id": "CLljSlYwFDuA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.5 Plotting Sentence-Level Sentiment Scores Over Time\n",
        "This code snippet visualizes the sentiment scores of a list of sentences over time. It uses the TextBlob library to compute the sentiment polarity of each sentence in the list. Then, using Matplotlib, a line plot is generated to visualize these sentiment scores. The x-axis of the plot represents the sequence number of each sentence, while the y-axis represents the corresponding sentiment score.\n",
        "\n",
        "The resulting plot provides a way to analyze the variation of sentiment in the whole text over time."
      ],
      "metadata": {
        "id": "vw4__FYu5CRC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First, install necessary packages\n",
        "\n",
        "\n",
        "# Import libraries\n",
        "import nltk\n",
        "from textblob import TextBlob\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Let's assume cleaned_sentences is your list of sentences\n",
        "# cleaned_sentences = [\"I am happy.\", \"I feel sad.\", \"Maybe I will feel better tomorrow.\", ...]\n",
        "\n",
        "# Initialize empty list to hold sentiment scores\n",
        "sentiment_scores = []\n",
        "\n",
        "# Loop over sentences and calculate sentiment\n",
        "for sentence in cleaned_sentences:\n",
        "    blob = TextBlob(sentence)\n",
        "    sentiment_scores.append(blob.sentiment.polarity)\n",
        "\n",
        "# Plot sentiment scores over time\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(sentiment_scores)\n",
        "plt.xlabel('Sentence Number')\n",
        "plt.ylabel('Sentiment Score')\n",
        "plt.title('Sentiment Over Time')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "CdUpJF2u5EuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4.6 Find Sentence by Index Number from Tokenized Text\n",
        "You can use this function to quickly retrieve specific sentences from the pre-tokenized text corpus, especially when you want to look up sentences for context or additional analysis."
      ],
      "metadata": {
        "id": "52nUIrRf5_lB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def search_sentence_by_index(index, cleaned_sentences):\n",
        "    if 0 <= index < len(cleaned_sentences):\n",
        "        return cleaned_sentences[index]\n",
        "    else:\n",
        "        return \"Index out of range.\"\n",
        "\n",
        "# Test the function\n",
        "print(search_sentence_by_index(0, cleaned_sentences))\n",
        "# '0' will show the first sentence. Replace '0'' with the index you want to look up from the previous outputs\n"
      ],
      "metadata": {
        "id": "UvBAUYsy6EOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4.8 Filtering and Ranking Sentences with Uncertainty Words by Sentiment Score\n",
        "\n",
        "This code snippet first defines a list of words indicating uncertainty, such as \"maybe,\" \"probably,\" \"might,\" etc. Then, it filters sentences from the cleaned_sentences list to include only those that contain at least one word from the uncertainty_words list. Along with each filtered sentence, its corresponding sentiment score is also retained.\n",
        "\n",
        "The filtered sentences are sorted based on their sentiment scores. The code then extracts the top 10 and bottom 10 sentences based on these scores. You can change this number if needed.\n",
        "\n",
        "Finally, it displays the top 10 sentences with the highest sentiment scores and the bottom 10 sentences with the lowest sentiment scores, all containing at least one word that indicates uncertainty.\n",
        "\n",
        "This could be useful for analyzing how uncertainty is framed in positive or negative contexts within a data corpus."
      ],
      "metadata": {
        "id": "Xek6U7Qp7Pn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Feel free to change the words found in the dictionary\n",
        "uncertainty_words = [\"maybe\", \"probably\", \"might\", \"perhaps\", \"unsure\", \"not sure\", \"I think\"]\n",
        "\n",
        "# Filter the sentences that contain at least one uncertainty word\n",
        "filtered_sentiment_sentence_pairs = [(score, sentence) for score, sentence in zip(sentiment_scores, cleaned_sentences) if any(word in sentence for word in uncertainty_words)]\n",
        "\n",
        "# Sort the list of tuples by the sentiment score\n",
        "sorted_pairs = sorted(filtered_sentiment_sentence_pairs, key=lambda x: x[0])\n",
        "\n",
        "# Extract the top 10 and bottom 10 sentences\n",
        "# These numbers can be changed to a longer list\n",
        "top_10_sentences = sorted_pairs[-10:]\n",
        "bottom_10_sentences = sorted_pairs[:10]\n",
        "\n",
        "# Display the top 10 sentences with highest sentiment scores\n",
        "print(\"Top 10 sentences with highest sentiment scores containing uncertainty words:\")\n",
        "for score, sentence in reversed(top_10_sentences):\n",
        "    print(f\"Score: {score}, Sentence: {sentence}\")\n",
        "\n",
        "# Display the bottom 10 sentences with lowest sentiment scores\n",
        "print(\"\\nBottom 10 sentences with lowest sentiment scores containing uncertainty words:\")\n",
        "for score, sentence in bottom_10_sentences:\n",
        "    print(f\"Score: {score}, Sentence: {sentence}\")\n"
      ],
      "metadata": {
        "id": "5RGioCsg7U5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question:\n",
        "Looking at the previous output. How do you think the computer comes up with these sentiment scores? DO you agree with them?"
      ],
      "metadata": {
        "id": "zczrOWcVGnKJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.9 Visualizing Sentiment Score Trends for Sentences with Uncertainty Words\n",
        "\n",
        "This code snippet visualizes the sentiment scores of sentences that contain words indicating uncertainty. The list is filtered to include only sentences that have uncertainty words. Using Matplotlib, the code then plots these sentiment scores against the sentence indices. The x-axis represents the index of each sentence in the filtered list, and the y-axis represents the sentiment score of each sentence.\n",
        "\n",
        "This visualization could help in understanding how sentiment varies over time in sentences that contain words of uncertainty."
      ],
      "metadata": {
        "id": "I4TyXrpo3NJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming `filtered_sentiment_sentence_pairs` contains tuples of (sentiment_score, sentence)\n",
        "# and you have already filtered this list to include only sentences containing uncertainty words\n",
        "\n",
        "# Extract just the sentiment scores\n",
        "filtered_sentiment_scores = [score for score, _ in filtered_sentiment_sentence_pairs]\n",
        "\n",
        "# Generate sentence indices for x-axis\n",
        "sentence_indices = list(range(1, len(filtered_sentiment_scores) + 1))\n",
        "\n",
        "# Create the plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(sentence_indices, filtered_sentiment_scores, marker='o', linestyle='-')\n",
        "plt.xlabel('Sentence Index')\n",
        "plt.ylabel('Sentiment Score')\n",
        "plt.title('Sentiment Score Over Time for Sentences Containing Uncertainty Words')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "uX0MZQZA3IHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conclusion\n",
        "\n",
        "Not all of these sentiment analysis are useful in looking at our uncertainty data. Can you think of other ways that we can incorporate sentiment analysis with this or other datasets?\n",
        "\n",
        "[Module 5](https://github.com/mrhallonline/NLP-Workshop/blob/main/Module_5_Basics_of_Text_Preprocessing_Workshop_Natural_Language_Toolkit_(NLTK)_V3.ipynb)"
      ],
      "metadata": {
        "id": "Re2LH78cC6eC"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}