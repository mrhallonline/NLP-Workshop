{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrhallonline/NLP-Workshop/blob/main/Module_3_Basic_analysis_and_Analysis_Workshop_Natural_Language_Toolkit_(NLTK)_V3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XI8uUvrQMkxD"
      },
      "source": [
        "## 3.0 Basic analysis  (20 minutes)\n",
        "In this module we will mess around with some of the basic features of NLTK. Looking at what types of information we can get from the features we have extracted.\n",
        "\n",
        "\n",
        "  * Basic Information\n",
        "    * Listing/Counting/Sorting/Ranking\n",
        "  * Working with NLTK Text Object Variable\n",
        "    * Concordance\n",
        "    * Similar\n",
        "    * n-grams/collocations\n",
        "    * Data Visualizations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.1 Reconnecting to the Text Corpus\n",
        "Since we have openened a new Colab notebook for this module, we will need to load the text file in Google Drive and extract the features again by recreating the variables from module 2. We will need to do something like this for each new module.\n",
        "\n",
        "*Click the code cells below to rerun the process for this module"
      ],
      "metadata": {
        "id": "IznhbRQnzQlw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import requests\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "# This is the full shared Drive link, the file ID starts at \"1i\" and ends at \"8S\"\n",
        "# https://docs.google.com/spreadsheets/d/1iJ4SG-QXfY4zw5K9B7Ununv3rb3iBj8S/edit?usp=drive_link&ouid=106477043869312333876&rtpof=true&sd=true\n",
        "\n",
        "# the file ID from the shareable link is pasted below in orange.\n",
        "file_id = \"1iJ4SG-QXfY4zw5K9B7Ununv3rb3iBj8S\"\n",
        "\n",
        "# construct the download URL, you would not need to change anything here.\n",
        "download_url = f\"https://docs.google.com/uc?export=download&id={file_id}\"\n",
        "\n",
        "# send a GET request to the download URL and save the response content\n",
        "response = requests.get(download_url)\n",
        "\n",
        "# The next line names the file after download. If you change it here, you will also need to change in the subsequent fields.\n",
        "# If you click on the folder icon in Colab you should see a file now appear called \"uncertaintyText.xlsx\"\n",
        "# These names can be changed to suit you own data\n",
        "with open(\"uncertaintyText.xlsx\", \"wb\") as f:\n",
        "    f.write(response.content)\n",
        "\n",
        "\n",
        "# Specify the path to the Excel file this where it was placed in 2.4 so that is the file and path you want to open\n",
        "excel_file_path = '/content/uncertaintyText.xlsx'\n",
        "\n",
        "# Specify the column name you want to pull the data corpus from\n",
        "column_name = 'transcript'\n",
        "\n",
        "# Read the Excel file and extract the specified column\n",
        "data = pd.read_excel(excel_file_path, engine='openpyxl')\n",
        "text_column = data[column_name]\n",
        "\n",
        "\n",
        "# Convert each item in the column to a string and then join them together to be saved as a text file containing all data in the transcript column.\n",
        "raw_uncertaintyText = ' '.join(map(str, text_column))\n",
        "\n",
        "\n",
        "# Save the string to a text file in your Google Drive\n",
        "with open('/content/raw_uncertaintyText.txt', 'w') as file:\n",
        "  file.write(raw_uncertaintyText)\n",
        "# load data from existing text file\n",
        "## this following path can be changed to access other text files that you may like to work with.\n",
        "filename = '/content/raw_uncertaintyText.txt'\n",
        "uncertaintyText = open(filename, 'rt', encoding='utf-8', errors='replace')\n",
        "\n",
        "# Extracting Different Features for analysis\n",
        "\n",
        "# The raw unchanged data from the text file now exists as a variable called \"raw_uncertaintyText\"\n",
        "raw_uncertaintyText = uncertaintyText.read()\n",
        "uncertaintyText.close()\n",
        "\n",
        "# 1. This line converts our raw text into word tokens\n",
        "# Regular expression tokenizing Gaps =False\n",
        "pattern = r'\\s+'\n",
        "uncertainty_wordTokens = nltk.regexp_tokenize(raw_uncertaintyText, pattern, gaps=True)\n",
        "\n",
        "# 2. This line converts our raw text into sentence tokens\n",
        "uncertainty_sentTokens = nltk.sent_tokenize(raw_uncertaintyText)\n",
        "\n",
        "# 3. This line converts our word tokens into NLTK text objects from the tokens\n",
        "uncertainty_wordTextObjects = nltk.Text(uncertainty_wordTokens)\n",
        "\n",
        "print(uncertainty_wordTokens)"
      ],
      "metadata": {
        "id": "vl2d-u7yU4Jo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"raw_uncertaintyText is a: \",type(raw_uncertaintyText))\n",
        "print(\"uncertainty_wordTokens is a: \",type(uncertainty_wordTokens))\n",
        "print(\"uncertainty_sentTokens is a: \",type(uncertainty_sentTokens))\n",
        "print(\"uncertainty_wordTextObjects is a: \",type(uncertainty_wordTextObjects))"
      ],
      "metadata": {
        "id": "5GJpXbUdwLg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.2 Interacting with our words tokens: uncertainty_wordTokens\n",
        "\n",
        "The following text cells will show different ways to look at the word tokens we have created. Just typing in the variable name and executing the code cell will show us a list of all the word tokens in the order that they appear.\n",
        "\n",
        "Current Variable List\n",
        "```\n",
        "raw_uncertaintyText\n",
        "uncertainty_wordTokens\n",
        "uncertainty_sentTokens\n",
        "uncertainty_wordTextObjects\n",
        "```\n",
        "If time permits, feel free to replace the current variable with one of the other ones to see how they differ structurally."
      ],
      "metadata": {
        "id": "gi8W2LqJcSvE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Executing the variable will show a list of the contents. Tokens are seperated by commas.\n",
        "uncertainty_wordTokens"
      ],
      "metadata": {
        "id": "zp95EgJqdQKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Placing the variable in a print function shows the contents as well as gives you a view that doesn't run down the page and allows you to do sorting and slicing of the contained data\n",
        "print(uncertainty_wordTokens)\n",
        "\n",
        "\n",
        "# The sorted set, lists and alphabetizes all words that appear at least once throughout the document.\n",
        "# The numbers at the end currently tell it to show the first 25 tokens. These numbers can be changed to look at different slices of data\n",
        "print(sorted(set(uncertainty_wordTokens[0:25])))"
      ],
      "metadata": {
        "id": "tqTlfXo-dZSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Execute code cell to see the total number of characters/tokens"
      ],
      "metadata": {
        "id": "ZPjikQqrcqqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# How many total word tokens\n",
        "len(uncertainty_wordTokens)"
      ],
      "metadata": {
        "id": "jb7XRPlY1Xt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Looking at the frequency distribution of our words"
      ],
      "metadata": {
        "id": "3Wiw_Iy7-FNb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05P5hdg7mwn3"
      },
      "outputs": [],
      "source": [
        "#Top 25 most common words with their counts\n",
        "fd = nltk.FreqDist(uncertainty_wordTokens)\n",
        "print(fd.most_common(25))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTWrxZ7we4iF"
      },
      "outputs": [],
      "source": [
        "#Top 25 most common words easier to read from page\n",
        "fd.tabulate(25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ci4n3j7I29Yq"
      },
      "outputs": [],
      "source": [
        "# Cumulative plot of top 25 words\n",
        "fd.plot(25, cumulative=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8w2BywIbudW"
      },
      "outputs": [],
      "source": [
        "# Create a frequency distribution\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "#fd = nltk.FreqDist(tokens_nltk_text)\n",
        "\n",
        "# Get the 10 most common words and their counts\n",
        "common = fd.most_common(10)\n",
        "\n",
        "# Unzip the words and counts into two separate lists\n",
        "words, counts = zip(*common)\n",
        "\n",
        "# Create a bar graph\n",
        "plt.bar(words, counts)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyO0jHoDNwTs"
      },
      "source": [
        "# 3.3 Working with NLTK Text Objects\n",
        "1. concordance\n",
        "2. similar words\n",
        "3. dispersion plots\n",
        "\n",
        "\n",
        "One of the variables that we created earlier was neither a string of words or a list of words, but was as labeled as a text object. What do we do with that bucket? We can work directly with some of NLTK's builtin functions. Such as concordance, similar, and dispersion plots."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#* concordance()\n",
        "The concordance function will search your entire data corpus for the existance of a specific word. It shows the word in question along with the surrounding words, providing context to help understand how the word is used in various contexts throughout the text. This can be particularly helpful for studying word usage patterns, exploring word meanings, or analyzing language usage in different contexts.You can see that the whole function itself is simply a merging of our variable name 'uncertainty_wordTextObjects' + the NLTK function 'concordance()'\n",
        "\n",
        "*Try changing the word found in line 4\n"
      ],
      "metadata": {
        "id": "zi_WMKJpi3ZT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yitbvU7wI870"
      },
      "outputs": [],
      "source": [
        "# This line will search for the occurences of one of our uncertainty words.\n",
        "# After running try using different uncertainty words\n",
        "# Feel free to change the number of lines that are shown as well\n",
        "uncertainty_wordTextObjects.concordance('think', lines = 25)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question:\n",
        "With your table partners come up with 3-4 other search terms that might give use some insight into what may be occuring during student moments of uncertainty. Look at the concordance for those words and talk about any insight gained."
      ],
      "metadata": {
        "id": "mFnnvaJz-y4y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#* similar()\n",
        "\n",
        "This NLTK function is used to find words that appear in a similar context as the given word in a text corpus. It looks at the context in which a word appears—specifically, the words that appear immediately before and after the target word—and finds other words that appear in the same or similar contexts.\n",
        "\n",
        "This can be useful for semantic analysis, linguistic exploration, and various natural language processing tasks. It allows us to see words that are used in similar ways and therefore might be considered similar in meaning within the text.\n",
        "\n"
      ],
      "metadata": {
        "id": "jlKlI3AgkybQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEgky9YxcczQ"
      },
      "outputs": [],
      "source": [
        "uncertainty_wordTextObjects.similar(\"maybe\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#* dispersion_plot()\n",
        "\n",
        "This function used to create a lexical dispersion plot, which visualizes the distribution of words in a text over the entirety of the data corpus. This plot helps you see where certain words occur throughout the text and whether there are any patterns or clusters of word occurrences.\n",
        "The dispersion plot displays a series of dashes or markers along a horizontal axis that represents the length of the text. Each dash or marker corresponds to the position of a specific word within the text. By visualizing the distribution of words in this way, you can quickly identify trends, repetition, and patterns of word usage."
      ],
      "metadata": {
        "id": "Q3UevIn2m_3I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEe6jT7pklF5"
      },
      "outputs": [],
      "source": [
        "# This will show a dispersion plot of the words of interest.\n",
        "# This is a great useful function to use in NLTK, but it does not seem to display correctly when utilized in Google Colab\n",
        "# You can see this if you compare the plot output below with our concordance search for \"maybe\"\n",
        "# It actually seems to list the words in reverse order to what is on the plot\n",
        "\n",
        "uncertainty_wordTextObjects.dispersion_plot([\"maybe\",\"help\", \"math\", \"think\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question:\n",
        "\n",
        "What are some inferences we can make using a lexical dispersion plot like this one?"
      ],
      "metadata": {
        "id": "LzU9FwGNCgZ6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyqhrJ7IOD-O"
      },
      "source": [
        "# 3.4 n-grams and collocations\n",
        "\n",
        "N-grams are contiguous sequences of words (or other linguistic units) that appear in a data corpus. They are able to show patterns and relationships or word usage in a text. For example bigrams are patterns of two words that appear together and trigrams are a pattern of three words appearing together.\n",
        "\n",
        "1. bigrams\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "toam7i9Gkvhu"
      },
      "outputs": [],
      "source": [
        "# Listing bigrams from data\n",
        "uncertainty_bigrams = list(nltk.bigrams(uncertainty_wordTextObjects))\n",
        "print(uncertainty_bigrams)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. trigrams"
      ],
      "metadata": {
        "id": "o7AQYSRezvNA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jUbcASUZwQT"
      },
      "outputs": [],
      "source": [
        "# Listing trigrams from data\n",
        "uncertainty_trigrams = list(nltk.trigrams(uncertainty_wordTextObjects))\n",
        "print(uncertainty_trigrams)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Collocations\n",
        "Collocations are pairs or groups of words that tend to appear frequently together in a text, often with a specific meaning or connotation ('fast food', 'red wine', 'United States of America').  They are not just random word combinations but rather indicative of language patterns. Identifying collocations can be useful for understanding the associations between words in a text.\n",
        "\n",
        "NLTK provides the collocations module, which includes methods to identify and extract collocations from a text. One common approach is to use the BigramAssocMeasures class along with the BigramCollocationFinder class to find significant word pairs (bigrams) based on various measures such as frequency among others."
      ],
      "metadata": {
        "id": "MpktUCiUsCO0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2zRzp6Zhd2v"
      },
      "outputs": [],
      "source": [
        "# display frequency of highest 50 bigrams\n",
        "# See if you can identify any bigrams that might signal uncertainty\n",
        "finder = nltk.collocations.BigramCollocationFinder.from_words(uncertainty_wordTextObjects)\n",
        "finder.ngram_fd.tabulate(50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEPNcD6W9K2o"
      },
      "outputs": [],
      "source": [
        "# display frequency of highest 25 trigrams\n",
        "finder = nltk.collocations.TrigramCollocationFinder.from_words(uncertainty_wordTextObjects)\n",
        "finder.ngram_fd.tabulate(25)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Any number of Ns\n",
        "\n",
        "If you have need to search for higher values of 'n', the following code cell when executed can search for any value of 'n' that you would like. Just change the number in line 4."
      ],
      "metadata": {
        "id": "7sRUwYVTrX8Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdyWCRp99YYT"
      },
      "outputs": [],
      "source": [
        "\n",
        "from nltk.util import ngrams\n",
        "\n",
        "n_value = 5  # Change this for different n values\n",
        "fourgrams = ngrams(uncertainty_wordTextObjects, n_value)\n",
        "\n",
        "# Tabulate the top n-grams\n",
        "fdist = nltk.FreqDist(fourgrams)\n",
        "fdist.tabulate(25)  # Top 10 fourgrams\n",
        "\n",
        "\n",
        "# Change the value for n and identify any other n-grams that might signal uncertainty"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finding words that appear 'close' to each other.\n",
        "The provided code snippet demonstrates how to search for instances of two specific words (e.g., \"I\" and \"think\") appearing within a certain distance from each other in a given text corpus. The code utilizes regular expressions and the NLTK library to tokenize the text, compile the regular expression pattern, and then search for matching sentences containing the specified word pair. It also includes a step to download the NLTK punkt tokenizer data if not already available, ensuring successful sentence tokenization.\n",
        "\n",
        "* You should hopefully notice here that NLTK is able to use both word and sentence tokens interactively with each other."
      ],
      "metadata": {
        "id": "uNYRSTzf2bEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "# Download the NLTK punkt tokenizer data\n",
        "nltk.download('punkt')\n",
        "#from nltk.sent_tokenize import sent_tokenize\n",
        "from nltk.text import Text\n",
        "import re\n"
      ],
      "metadata": {
        "id": "mhCgx_AB3auS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can change the words in line 9 and 10 to investigate different word combinations. Line 13 allows you to chage the maximum distance between the words."
      ],
      "metadata": {
        "id": "vo1cg3itAF4g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Tokenize the text into words\n",
        "corpus_words = nltk.word_tokenize(raw_uncertaintyText)\n",
        "\n",
        "# Create a Text object from the tokenized words\n",
        "corpus_text = Text(corpus_words)\n",
        "\n",
        "# Define the two words to search for\n",
        "word1 = \"I\"\n",
        "word2 = \"think\"\n",
        "\n",
        "# Define the maximum distance between the two words\n",
        "max_distance = 10  # Adjust this value as needed\n",
        "\n",
        "# Create a pattern for searching\n",
        "pattern = r\"\\b\" + word1 + r\"\\W+(?:\\w+\\W+){\" + f\"0,{max_distance}\" + r\"}\" + word2 + r\"\\b\"\n",
        "\n",
        "# Extract plain text from the corpus_text object\n",
        "plain_text = ' '.join(corpus_text.tokens)\n",
        "\n",
        "# Compile the regular expression\n",
        "search_regex = re.compile(pattern, re.IGNORECASE)\n",
        "\n",
        "# Find sentences where the pattern appears\n",
        "sentences = nltk.sent_tokenize(plain_text)\n",
        "\n",
        "# Search for sentences where the two words appear within the specified distance\n",
        "matching_sentences = []\n",
        "for sentence in sentences:\n",
        "    if search_regex.search(sentence):\n",
        "        matching_sentences.append(sentence)\n",
        "\n",
        "# Print the matching sentences\n",
        "for sentence in matching_sentences:\n",
        "    print(sentence)\n"
      ],
      "metadata": {
        "id": "GZem2Xwx4k9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAN46LK6q-qZ"
      },
      "source": [
        "# End of Module 3\n",
        "Question:\n",
        "Work with your table partners and come up with 3-4 other useful word combinations that could give us insight into our uncertainty data corpus. For example \"did\" and \"you\", do we find examples of students asking questions about the process used by other students?\n",
        "\n",
        "[Module 4](https://github.com/mrhallonline/NLP-Workshop/blob/main/Module_4_Running_Basic_Sentiment_Analysis_Workshop_Natural_Language_Toolkit_(NLTK)_V3.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A huge takeaway to consider at this point is that we still have quite a bit of noise contained in our data and that shows up in our analysis\n",
        "\n",
        "for example:\n",
        "\n",
        "Our second and third highest frequency collocation containing three words are the following ('I', \"don't\", 'know') ('I', \"don't\", 'know.')\n",
        "** in this case \"know\" and \"know.\" are identified as being two different words which is no doubt causing underlying issues with other \"words\" in our data corpus\n",
        "\n",
        "Module 5 will allow you to spend time interacting more deeply with text preprocessing allowing you to much better extract the specific features that you need from a transcript corpus. Seeing how the signal interacts with the noise is helpful in planning and better understanding the importance of text preprocessing and feature extraction from language."
      ],
      "metadata": {
        "id": "jmHyWx9ICMhj"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}