{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrhallonline/NLP-Workshop/blob/main/Module_3_Basic_analysis_and_Analysis_Workshop_Natural_Language_Toolkit_(NLTK)_V3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XI8uUvrQMkxD"
      },
      "source": [
        "## 3.0 Basic analysis  (20 minutes)\n",
        "In this module we will mess around with some of the basic features of NLTK. Looking at what types of information we can get from the features we have extracted.\n",
        "\n",
        "\n",
        "  * Basic Information\n",
        "    * Listing/Counting/Sorting/Ranking\n",
        "  * Working with NLTK Text Object Variable\n",
        "    * Concordance\n",
        "    * Similar\n",
        "    * n-grams/collocations\n",
        "    * Data Visualizations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.1 Reconnecting to Google Drive\n",
        "Since we have openened a new Colab notebook for this module, we will need to remount Google Drive and extract the features again by recreating the variables from module 2. We will need to do something like this for each new module.\n",
        "\n",
        "*Click the two code cells below to rerun the process"
      ],
      "metadata": {
        "id": "IznhbRQnzQlw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4zpU53T5wRhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-creating variables from module 2\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "# load data from existing text file\n",
        "filename = '/content/drive/MyDrive/raw_uncertaintyText.txt'\n",
        "uncertaintyText = open(filename, 'rt', encoding='utf-8', errors='replace')\n",
        "\n",
        "raw_uncertaintyText = uncertaintyText.read()\n",
        "uncertaintyText.close()\n",
        "\n",
        "# Word Tokenization\n",
        "#uncertainty_wordTokens = nltk.word_tokenize(raw_uncertaintyText)\n",
        "\n",
        "# Regular expression tokenizing Gaps =False\n",
        "pattern = r'\\s+'\n",
        "uncertainty_wordTokens = nltk.regexp_tokenize(raw_uncertaintyText, pattern, gaps=True)\n",
        "\n",
        "# This line converts our raw text into sentence tokens\n",
        "uncertainty_sentTokens = nltk.sent_tokenize(raw_uncertaintyText)\n",
        "\n",
        "# Creating a Text object from the tokens\n",
        "uncertainty_wordTextObjects = nltk.Text(uncertainty_wordTokens)\n",
        "\n",
        "print(\"raw_uncertaintyText is a: \",type(raw_uncertaintyText))\n",
        "print(\"uncertainty_wordTokens is a: \",type(uncertainty_wordTokens))\n",
        "print(\"uncertainty_sentTokens is a: \",type(uncertainty_sentTokens))\n",
        "print(\"uncertainty_wordTextObjects is a: \",type(uncertainty_wordTextObjects))"
      ],
      "metadata": {
        "id": "5GJpXbUdwLg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7_V0iG6kwxK"
      },
      "source": [
        "# 3.2 Interacting with our words:\n",
        "uncertainty_wordTokens"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following text cells will exhibited different ways to look at the word tokens we have created. Just typing in the variable name and executing the code cell will show us a list of all the word tokens in the order that they appear.\n",
        "\n",
        "Current Variable List\n",
        "```\n",
        "raw_uncertaintyText\n",
        "uncertainty_wordTokens\n",
        "uncertainty_sentTokens\n",
        "uncertainty_wordTextObjects\n",
        "```\n",
        "Replace the current variable with one of the other ones to see how they differ structurally."
      ],
      "metadata": {
        "id": "gi8W2LqJcSvE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Executing the variable will show the contents\n",
        "uncertainty_wordTokens"
      ],
      "metadata": {
        "id": "zp95EgJqdQKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Placing the variable in a print function shows the contents as well as gives you a view that doesn't run down the page and allows you to do sorting and slicing of the contained data\n",
        "print(uncertainty_wordTokens)\n",
        "\n",
        "\n",
        "# The sorted set, lists and alphabetizes all words that appear at least once throughout the document.\n",
        "# The numbers at the end currently tell it to show the first 25 tokens. These numbers can be changed to look at different slices of data\n",
        "print(sorted(set(uncertainty_wordTokens[0:25])))"
      ],
      "metadata": {
        "id": "tqTlfXo-dZSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execute code cell to see the total number of characters/tokens"
      ],
      "metadata": {
        "id": "ZPjikQqrcqqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# How many total word tokens\n",
        "len(uncertainty_wordTokens)"
      ],
      "metadata": {
        "id": "jb7XRPlY1Xt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05P5hdg7mwn3"
      },
      "outputs": [],
      "source": [
        "#Top 25 most common words with their counts\n",
        "fd = nltk.FreqDist(uncertainty_wordTokens)\n",
        "print(fd.most_common(25))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTWrxZ7we4iF"
      },
      "outputs": [],
      "source": [
        "#Top 25 most common words easier to read from page\n",
        "fd.tabulate(25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ci4n3j7I29Yq"
      },
      "outputs": [],
      "source": [
        "# Cumulative plot of top 25 words\n",
        "fd.plot(25, cumulative=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8w2BywIbudW"
      },
      "outputs": [],
      "source": [
        "# Create a frequency distribution\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "#fd = nltk.FreqDist(tokens_nltk_text)\n",
        "\n",
        "# Get the 10 most common words and their counts\n",
        "common = fd.most_common(10)\n",
        "\n",
        "# Unzip the words and counts into two separate lists\n",
        "words, counts = zip(*common)\n",
        "\n",
        "# Create a bar graph\n",
        "plt.bar(words, counts)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyO0jHoDNwTs"
      },
      "source": [
        "# 3.3 Working with NLTK Text Objects\n",
        "1. concordance\n",
        "2. similar words\n",
        "3. dispersion plots\n",
        "\n",
        "\n",
        "One of the variables that we created earlier was neither a string of words or a list of words, but was as labeled as a text object. What do we do with that bucket? We can work directly with some of NLTK's builtin functions. Such as concordance, similar, and dispersion plots."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#* concordance()\n",
        "The concordance function will search your entire data corpus for the existance of a specific word. It shows the word in question along with the surrounding words, providing context to help understand how the word is used in various contexts throughout the text. This can be particularly helpful for studying word usage patterns, exploring word meanings, or analyzing language usage in different contexts.You can see that the whole function itself is simply a merging of our variable name 'uncertainty_wordTextObjects' + the NLTK function 'concordance()'\n",
        "\n",
        "*Try seeing if this will work with any of the other variables that we created so far.\n"
      ],
      "metadata": {
        "id": "zi_WMKJpi3ZT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yitbvU7wI870"
      },
      "outputs": [],
      "source": [
        "# This line will search for the occurences of one of our uncertainty words.\n",
        "# After running try using different uncertainty words\n",
        "# Feel free to change the number of lines that are shown as well\n",
        "uncertainty_wordTextObjects.concordance('maybe', lines = 15)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#* similar()\n",
        "\n",
        "This NLTK function is used to find words that appear in a similar context as the given word in a text corpus. It looks at the context in which a word appears—specifically, the words that appear immediately before and after the target word—and finds other words that appear in the same or similar contexts.\n",
        "\n",
        "This can be useful for semantic analysis, linguistic exploration, and various natural language processing tasks. It allows us to see words that are used in similar ways and therefore might be considered similar in meaning within the text.\n",
        "\n"
      ],
      "metadata": {
        "id": "jlKlI3AgkybQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEgky9YxcczQ"
      },
      "outputs": [],
      "source": [
        "uncertainty_wordTextObjects.similar(\"maybe\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#* dispersion_plot()\n",
        "\n",
        "This function used to create a lexical dispersion plot, which visualizes the distribution of words in a text over the entirety of the data corpus. This plot helps you see where certain words occur throughout the text and whether there are any patterns or clusters of word occurrences.\n",
        "The dispersion plot displays a series of dashes or markers along a horizontal axis that represents the length of the text. Each dash or marker corresponds to the position of a specific word within the text. By visualizing the distribution of words in this way, you can quickly identify trends, repetition, and patterns of word usage."
      ],
      "metadata": {
        "id": "Q3UevIn2m_3I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEe6jT7pklF5"
      },
      "outputs": [],
      "source": [
        "# This will show a dispersion plot of the words of interest.\n",
        "# This is a great useful function to use in NLTK, but it does not seem to work correctly when utilized in Google Colab\n",
        "# You can see this if you compare the plot output below with our concordance search for \"maybe\"\n",
        "\n",
        "uncertainty_wordTextObjects.dispersion_plot([\"maybe\",\"help\", \"math\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyqhrJ7IOD-O"
      },
      "source": [
        "# 3.4 n-grams and collocations\n",
        "\n",
        "N-grams are contiguous sequences of words (or other linguistic units) that appear in a data corpus. They are able to show patterns and relationships or word usage in a text. For example bigrams are patterns of two words that appear together and trigrams are a pattern of three words appearing together.\n",
        "\n",
        "1. bigrams\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "toam7i9Gkvhu"
      },
      "outputs": [],
      "source": [
        "# Listing bigrams from data\n",
        "uncertainty_bigrams = list(nltk.bigrams(uncertainty_wordTextObjects))\n",
        "print(uncertainty_bigrams)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. trigrams"
      ],
      "metadata": {
        "id": "o7AQYSRezvNA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jUbcASUZwQT"
      },
      "outputs": [],
      "source": [
        "# Listing trigrams from data\n",
        "uncertainty_trigrams = list(nltk.trigrams(uncertainty_wordTextObjects))\n",
        "print(uncertainty_trigrams)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Collocations\n",
        "Collocations are pairs or groups of words that tend to appear frequently together in a text, often with a specific meaning or connotation ('fast food', 'red wine', 'United States of America').  They are not just random word combinations but rather indicative of language patterns. Identifying collocations can be useful for understanding the associations between words in a text.\n",
        "\n",
        "NLTK provides the collocations module, which includes methods to identify and extract collocations from a text. One common approach is to use the BigramAssocMeasures class along with the BigramCollocationFinder class to find significant word pairs (bigrams) based on various measures such as frequency among others."
      ],
      "metadata": {
        "id": "MpktUCiUsCO0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2zRzp6Zhd2v"
      },
      "outputs": [],
      "source": [
        "# display frequency of highest 25 bigrams\n",
        "finder = nltk.collocations.BigramCollocationFinder.from_words(uncertainty_wordTextObjects)\n",
        "finder.ngram_fd.tabulate(25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEPNcD6W9K2o"
      },
      "outputs": [],
      "source": [
        "# display frequency of highest 25 trigrams\n",
        "finder = nltk.collocations.TrigramCollocationFinder.from_words(uncertainty_wordTextObjects)\n",
        "finder.ngram_fd.tabulate(25)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Any number of Ns\n",
        "\n",
        "If you have need to search for higher values of 'n', the following code cell when executed can search for any value of 'n' that you would like. Just change the number in line 4."
      ],
      "metadata": {
        "id": "7sRUwYVTrX8Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdyWCRp99YYT"
      },
      "outputs": [],
      "source": [
        "\n",
        "from nltk.util import ngrams\n",
        "\n",
        "n_value = 4  # Change this for different n values\n",
        "fourgrams = ngrams(uncertainty_wordTextObjects, n_value)\n",
        "\n",
        "# Tabulate the top n-grams\n",
        "fdist = nltk.FreqDist(fourgrams)\n",
        "fdist.tabulate(25)  # Top 10 fourgrams\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finding words that appear 'close' to each other.\n",
        "The provided code snippet demonstrates how to search for instances of two specific words (e.g., \"I\" and \"think\") appearing within a certain distance from each other in a given text corpus. The code utilizes regular expressions and the NLTK library to tokenize the text, compile the regular expression pattern, and then search for matching sentences containing the specified word pair. It also includes a step to download the NLTK punkt tokenizer data if not already available, ensuring successful sentence tokenization.\n",
        "\n",
        "* You should hopefully notice here that NLTK is able to use both word and sentence tokens interactively with each other."
      ],
      "metadata": {
        "id": "uNYRSTzf2bEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "# Download the NLTK punkt tokenizer data\n",
        "nltk.download('punkt')\n",
        "#from nltk.sent_tokenize import sent_tokenize\n",
        "from nltk.text import Text\n",
        "import re\n"
      ],
      "metadata": {
        "id": "mhCgx_AB3auS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the text into words\n",
        "corpus_words = nltk.word_tokenize(raw_uncertaintyText)\n",
        "\n",
        "# Create a Text object from the tokenized words\n",
        "corpus_text = Text(corpus_words)\n",
        "\n",
        "# Define the two words to search for\n",
        "word1 = \"I\"\n",
        "word2 = \"think\"\n",
        "\n",
        "# Define the maximum distance between the two words\n",
        "max_distance = 10  # Adjust this value as needed\n",
        "\n",
        "# Create a pattern for searching\n",
        "pattern = r\"\\b\" + word1 + r\"\\W+(?:\\w+\\W+){\" + f\"0,{max_distance}\" + r\"}\" + word2 + r\"\\b\"\n",
        "\n",
        "# Extract plain text from the corpus_text object\n",
        "plain_text = ' '.join(corpus_text.tokens)\n",
        "\n",
        "# Compile the regular expression\n",
        "search_regex = re.compile(pattern, re.IGNORECASE)\n",
        "\n",
        "# Find sentences where the pattern appears\n",
        "sentences = sent_tokenize(plain_text)\n",
        "\n",
        "# Search for sentences where the two words appear within the specified distance\n",
        "matching_sentences = []\n",
        "for sentence in sentences:\n",
        "    if search_regex.search(sentence):\n",
        "        matching_sentences.append(sentence)\n",
        "\n",
        "# Print the matching sentences\n",
        "for sentence in matching_sentences:\n",
        "    print(sentence)\n"
      ],
      "metadata": {
        "id": "GZem2Xwx4k9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.5 Activity if time permitting\n",
        "The following interactive tool will ask for a word plus a number for n and will report back with a plot of the most frequent ngrams containing the word entered.\n",
        "\n",
        "See if this tool or any of the earlier code snippet can be useful for any of the following analysis\n",
        "\n",
        "* Common themes or topics discussed.\n",
        "* Questions posed by students indicating uncertainty (using the words we discussed earlier like \"maybe\", \"think\" etc.).\n",
        "\n",
        "Discuss the findings with your table partner."
      ],
      "metadata": {
        "id": "Q1j0y1_qsOdW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import ngrams\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sample Text Corpus (replace this with your own)\n",
        "text_corpus = raw_uncertaintyText\n",
        "\n",
        "# Download required datasets\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Function to get n-grams\n",
        "def get_ngrams(text, n):\n",
        "    words = word_tokenize(text.lower())\n",
        "    generated_ngrams = ngrams(words, n)\n",
        "    return generated_ngrams\n",
        "\n",
        "# Function to interactively explore n-grams\n",
        "def interactive_ngram_explorer():\n",
        "    uncertainty_word = input(\"Enter the word of uncertainty you're interested in (e.g., maybe, think, etc.): \").lower()\n",
        "    n_value = int(input(\"Enter the n-gram length (e.g., 2 for bigrams, 3 for trigrams, etc.): \"))\n",
        "\n",
        "    n_grams = list(get_ngrams(text_corpus, n_value))\n",
        "    filtered_ngrams = [gram for gram in n_grams if uncertainty_word in gram]\n",
        "    ngram_frequencies = Counter(filtered_ngrams)\n",
        "\n",
        "    print(f\"\\nHere are the most common {n_value}-grams containing the word '{uncertainty_word}':\")\n",
        "    for gram, freq in ngram_frequencies.items():\n",
        "        print(f\"{gram}: {freq}\")\n",
        "\n",
        "    # Optional: plot the results\n",
        "    items = ngram_frequencies.items()\n",
        "    labels = [str(i[0]) for i in items]\n",
        "    values = [i[1] for i in items]\n",
        "\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.barh(labels, values)\n",
        "    plt.xlabel('Frequency')\n",
        "    plt.title(f\"Most common {n_value}-grams containing the word '{uncertainty_word}'\")\n",
        "    plt.show()\n",
        "\n",
        "# Run the interactive explorer\n",
        "interactive_ngram_explorer()\n"
      ],
      "metadata": {
        "id": "oNU0rysMA03g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAN46LK6q-qZ"
      },
      "source": [
        "# End of Module 3\n",
        "\n",
        "A huge takeaway to consider at this point is that we still have quite a bit of noise contained in our data and that shows up in our analysis\n",
        "\n",
        "for example:\n",
        "* Our second and third highest frequency collocation containing three words are the following ('I', \"don't\", 'know')   ('I', \"don't\", 'know.')\n",
        "** in this case \"know\" and \"know.\" are identified as being two different words which is no doubt causing underlying issues with other \"words\" in our data corpus\n",
        "\n",
        "Module 5 will allow you to spend time interacting more deeply with text preprocessing allowing you to much better extract the specific features that you need from a transcript corpus. Seeing how the signal interacts with the noise is helpful in planning and better understanding the importance of text preprocessing and feature extraction from language."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}