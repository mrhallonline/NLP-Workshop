{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrhallonline/NLP-Workshop/blob/main/Module_5_Basics_of_Text_Preprocessing_Workshop_Natural_Language_Toolkit_(NLTK)_V3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.0 Text Wrangling and Basic Preprocessing (20 Minutes)\n",
        "* 5.1 Reconnecting to Google Drive and basic processing\n",
        "* 5.2 Tokenizing Text Using Different Regular Expressions\n",
        "* 5.3 All-in-One Text Normalization Using NLTK\n",
        "* 5.4 Viewing the List of English Stopwords using NLTK\n",
        "* 5.5 List of Ways to Extract Features Using Regular Expression as the tokenizer\n",
        "* 5.6 Conclusion\n",
        "  * Issues to keep in mind when normalizing your data corpus\n",
        "  * Ethical considerations/Further usage\n",
        "  * Wrap-up"
      ],
      "metadata": {
        "id": "wwL3SA-XgE8O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5.1 Reconnecting to Google Drive and basic processing\n",
        "More or less same as before just to get things started up again."
      ],
      "metadata": {
        "id": "RP6ubIMLepxl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# load data from existing text file we created in module 1\n",
        "filename = '/content/drive/MyDrive/raw_uncertaintyText.txt'\n",
        "uncertaintyText = open(filename, 'rt', encoding='utf-8', errors='replace')\n",
        "\n",
        "raw_uncertaintyText = uncertaintyText.read()\n",
        "uncertaintyText.close()\n",
        "\n",
        "# Word Tokenization\n",
        "#uncertainty_wordTokens = nltk.word_tokenize(raw_uncertaintyText)\n",
        "\n",
        "# Regular expression word tokenizing\n",
        "pattern = r'\\s+'\n",
        "uncertainty_wordTokens = nltk.regexp_tokenize(raw_uncertaintyText, pattern, gaps=True)\n",
        "\n",
        "# This line converts our raw text into sentence tokens\n",
        "uncertainty_sentTokens = nltk.sent_tokenize(raw_uncertaintyText)\n",
        "\n",
        "# Creating a Text object from the tokens\n",
        "uncertainty_wordTextObjects = nltk.Text(uncertainty_wordTokens)\n",
        "\n",
        "print(\"raw_uncertaintyText is a: \",type(raw_uncertaintyText))\n",
        "print(\"uncertainty_wordTokens is a: \",type(uncertainty_wordTokens))\n",
        "print(\"uncertainty_sentTokens is a: \",type(uncertainty_sentTokens))\n",
        "print(\"uncertainty_wordTextObjects is a: \",type(uncertainty_wordTextObjects))"
      ],
      "metadata": {
        "id": "O3376wFCekn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwN-_b4jJkGO"
      },
      "source": [
        "# 5.2 Tokenizing Text Using Different Regular Expressions\n",
        "This code snippet demonstrates different ways of tokenizing text using regular expressions via the NLTK library. The text is tokenized based on word boundaries, whitespace, and capitalized words using different configurations of the nltk.regexp_tokenize() function.**bold text**\n",
        "\n",
        "* text_regexTokensWF: Tokenizes the text based on word boundaries, with gaps=False. This will create tokens that match the given pattern, essentially capturing the words themselves.\n",
        "\n",
        "* text_regexTokensST: Tokenizes the text based on white spaces, with gaps=True. This captures the gaps between tokens, essentially tokenizing by spaces.\n",
        "\n",
        "* text_regexTokensSF: Tokenizes the text based on white spaces, with gaps=False. This captures tokens that are separated by spaces.\n",
        "\n",
        "* text_regexTokensCAPS: Tokenizes the text based on capitalized words. This will create tokens that start with a capital letter followed by one or more word characters.\n",
        "\n",
        "The lengths and first 200 characters of each tokenized list are printed out to offer a quick view of the tokenization results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTfkERILJmZ4"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "#Regex Gaps=True\n",
        "pattern2 = r'\\w+'\n",
        "text_regexTokensWF = nltk.regexp_tokenize(raw_uncertaintyText, pattern2, gaps=False)\n",
        "\n",
        "# Regular expression tokenizing Gaps =False\n",
        "pattern3 = r'\\s+'\n",
        "text_regexTokensST = nltk.regexp_tokenize(raw_uncertaintyText, pattern3, gaps=True)\n",
        "\n",
        "\n",
        "#Regex Gaps=True\n",
        "pattern4 = r'\\s+'\n",
        "text_regexTokensSF = nltk.regexp_tokenize(raw_uncertaintyText, pattern4, gaps=False)\n",
        "\n",
        "#Words with capital letters\n",
        "pattern5 = r'[A-Z]\\w+'\n",
        "text_regexTokensCAPS = nltk.regexp_tokenize(raw_uncertaintyText, pattern5)\n",
        "\n",
        "#Contractions (can't, won't)\n",
        "pattern10 = r'\\b\\w+\\'\\w+\\b'\n",
        "text_regexTokens10 = nltk.regexp_tokenize(raw_uncertaintyText, pattern10)\n",
        "\n",
        "print(\"text_regexTokensWF Length =\", len(text_regexTokensWF),\" Characters\", text_regexTokensWF[0:200])\n",
        "print(\"text_regexTokensST Length =\", len(text_regexTokensST),\" Characters\", text_regexTokensST[0:200])\n",
        "print(\"text_regexTokensSF Length = \", len(text_regexTokensSF),\" Characters\", text_regexTokensSF[0:200])\n",
        "print(\"text_regexTokensCAPS Length = \", len(text_regexTokensCAPS),\" Characters\", text_regexTokensCAPS[0:200])\n",
        "print(\"text_regexTokens10 Length = \", len(text_regexTokens10),\" Characters\", text_regexTokens10[0:200])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.3 All-in-One Text Normalization Using NLTK\n",
        "\n",
        "This code snippet performs multiple steps to normalize a text corpus using NLTK in Python.\n",
        "\n",
        "* Convert to Lowercase: The tokens previously obtained (referred to as text_regexTokensST in the code) are converted to lowercase using list comprehension.\n",
        "\n",
        "* Remove Punctuation: Punctuation marks are removed from each token. This is done by translating each string using a translation table that removes punctuation marks.\n",
        "\n",
        "* Retain Alphabetic Words: Words that are not purely alphabetic are filtered out to keep only words containing alphabetic characters.\n",
        "\n",
        "* Remove Stopwords: Finally, common stopwords like 'and', 'the', etc., are removed from the list of tokens. The NLTK library provides a list of such stopwords for the English language.\n",
        "\n",
        "The resulting list, cleaned_uncertainty_wordTokens, contains tokens that are lowercase, clear of punctuation, alphabetic, and are not stopwords. The first 200 tokens of this list are printed for you to compare to the earlier datasets."
      ],
      "metadata": {
        "id": "1I4AhDFYuizN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZZACFNXfuet"
      },
      "outputs": [],
      "source": [
        "#Combined All in one normalizer\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# convert to lower case\n",
        "lower_tokens = [w.lower() for w in text_regexTokensST]\n",
        "\n",
        "# remove punctuation from each word\n",
        "import string\n",
        "table = str.maketrans('', '', string.punctuation)\n",
        "stripped = [w.translate(table) for w in lower_tokens]\n",
        "\n",
        "\n",
        "# remove remaining tokens that are not alphabetic\n",
        "alpha_words = [word for word in stripped if word.isalpha()]\n",
        "\n",
        "\n",
        "# filter out stop words\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "cleaned_uncertainty_wordTokens = [w for w in alpha_words if not w in stop_words]\n",
        "print(cleaned_uncertainty_wordTokens[0:200])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.4 Viewing the List of English Stopwords using NLTK\n",
        "This snippet will print the list of English stopwords that NLTK uses. The stopwords are common words such as 'and', 'the', 'is', etc., that are often filtered out in text processing to extract more 'meaningful' language for analysis. Keep in mind that you can change the words that appear in the list if needed."
      ],
      "metadata": {
        "id": "-1qYntVavbeZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download the stopwords package if you haven't already\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Get English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Print the list of stopwords\n",
        "print(sorted(list(stop_words)))\n"
      ],
      "metadata": {
        "id": "4GJ_8pYMvbpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.5 List of Ways to Extract Features Using Regular Expression as the tokenizer\n",
        "This code uses the NLTK to tokenize text (raw_uncertaintyText) using a variety of regular expression patterns. These patterns include filtering for\n",
        "1. word tokens\n",
        "2. non-word tokens\n",
        "3. words with capital letters\n",
        "4. sentences\n",
        "5. email addresses\n",
        "6. digits\n",
        "7. URLs\n",
        "8. abbreviations\n",
        "9. words with hyphens\n",
        "10. contractions\n",
        "11. hashtags\n",
        "12. parenthesized expressions\n",
        "13. phone numbers\n",
        "\n",
        "Each pattern is applied to the text, and the resulting tokens are stored in separate variables.\n",
        "\n",
        "The code is a demonstration of multiple regular expression patterns that can be used to tokenize text for different needs. This code is useful for initial data preparation steps where you may need to extract specific types of tokens from raw text."
      ],
      "metadata": {
        "id": "2kkvW3ssflAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Word Tokens\n",
        "pattern1 = r'\\w+'\n",
        "text_regexTokens1 = nltk.regexp_tokenize(raw_uncertaintyText, pattern1)\n",
        "\n",
        "#Non-Word Tokens\n",
        "pattern2 = r'\\W+'\n",
        "text_regexTokens2 = nltk.regexp_tokenize(raw_uncertaintyText, pattern2)\n",
        "\n",
        "#Words with capital letters\n",
        "pattern3 = r'[A-Z]\\w+'\n",
        "text_regexTokens3 = nltk.regexp_tokenize(raw_uncertaintyText, pattern3)\n",
        "\n",
        "#Sentences\n",
        "pattern4 = r'[A-Z][^.!?]*[.!?]'\n",
        "text_regexTokens4 = nltk.regexp_tokenize(raw_uncertaintyText, pattern4)\n",
        "\n",
        "#Email Addresses\n",
        "pattern5 = r'\\S+@\\S+'\n",
        "text_regexTokens = nltk.regexp_tokenize(raw_uncertaintyText, pattern5)\n",
        "\n",
        "#Digits\n",
        "pattern6 = r'\\d+'\n",
        "text_regexTokens6 = nltk.regexp_tokenize(raw_uncertaintyText, pattern6)\n",
        "\n",
        "#URLs\n",
        "pattern7 = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
        "text_regexTokens7 = nltk.regexp_tokenize(raw_uncertaintyText, pattern7)\n",
        "\n",
        "#Custom Patterns - Abbreviations with following period\n",
        "pattern8 = r'\\b[A-Za-z]\\.(\\s)?'\n",
        "text_regexTokens8 = nltk.regexp_tokenize(raw_uncertaintyText, pattern8)\n",
        "\n",
        "\n",
        "#Words with hyphens non-violent\n",
        "pattern9 = r'\\w+(?:-\\w+)*'\n",
        "text_regexTokens9 = nltk.regexp_tokenize(raw_uncertaintyText, pattern9)\n",
        "\n",
        "\n",
        "#Contractions (can't, won't)\n",
        "pattern10 = r'\\b\\w+\\'\\w+\\b'\n",
        "text_regexTokens10 = nltk.regexp_tokenize(raw_uncertaintyText, pattern10)\n",
        "\n",
        "#Hashtags\n",
        "pattern11 = r'#\\w+'\n",
        "text_regexTokens11 = nltk.regexp_tokenize(raw_uncertaintyText, pattern11)\n",
        "\n",
        "\n",
        "#Custom Pattern- Parenthesized expressions\n",
        "pattern12 = r'\\(.*?\\)'\n",
        "text_regexTokens12 = nltk.regexp_tokenize(raw_uncertaintyText, pattern12)\n",
        "\n",
        "#Custom Pattern - Phone number\n",
        "pattern13 = r'\\d{3}-\\d{3}-\\d{4}'\n",
        "text_regexTokens13 = nltk.regexp_tokenize(raw_uncertaintyText, pattern13)"
      ],
      "metadata": {
        "id": "pU1iiOdpfjBr"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "## Issues to keep in mind when running the all-in-one text normalization process:\n",
        "\n",
        "* Context Loss: Removing stopwords and punctuation can sometimes result in the loss of context or meaning, which could be important for certain types of analysis like sentiment analysis or named entity recognition.\n",
        "\n",
        "* Non-English Text: This code is tailored for English text. If your dataset contains text in other languages, you would need to adjust the list of stopwords and possibly other aspects of the normalization process.\n",
        "\n",
        "* Special Tokens: Some tokens might be specialized terms or jargon that should not be lower-cased or stripped of punctuation. This would need special handling.\n",
        "\n",
        "* Homonyms: Converting all words to lowercase might make some words that are spelled the same but have different meanings indistinguishable. For example, \"US\" (United States) and \"us\" (pronoun) would both become \"us.\"\n",
        "\n",
        "* Information Loss: By removing all words that are not purely alphabetic, you might lose some potentially important tokens like numerals or alphanumeric codes which might be significant in a particular context.\n",
        "\n",
        "## Potential Pitfalls & Ethical Considerations\n",
        "\n",
        "* Challenges of NLP and potential misinterpretations.\n",
        "* Context is important in qualitative research NLP can only aid, not replace, human analysis.\n",
        "* Important to take these analyses in the broader context of classroom dynamics, pedagogical approaches, and individual student backgrounds. Machine analysis can offer insights but can't replace the nuanced understanding of an educator\n",
        "* Ethical considerations, especially when analyzing students' data..\n",
        "* Importance of consent when analyzing student discussions and potential implications of the findings.\n",
        "* The key is to use NLTK and Python as tools to augment qualitative analysis, not replace it. They can help identify patterns or areas of interest, but the rich, nuanced understanding and interpretation will come from the qualitative researcher’s expertise."
      ],
      "metadata": {
        "id": "9E0gw8oTv4r3"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}