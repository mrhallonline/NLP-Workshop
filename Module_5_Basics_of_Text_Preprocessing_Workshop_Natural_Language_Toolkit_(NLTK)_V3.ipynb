{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrhallonline/NLP-Workshop/blob/main/Module_5_Basics_of_Text_Preprocessing_Workshop_Natural_Language_Toolkit_(NLTK)_V3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.0 Basics of Text Preprocessing\n",
        "* 5.1 Tokenization\n",
        "* 5.2 Stop words removal\n",
        "* 5.3 Stemming\n",
        "* 5.4 Lemmatizing"
      ],
      "metadata": {
        "id": "wwL3SA-XgE8O"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwN-_b4jJkGO"
      },
      "source": [
        "# 5.1 Regex Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTfkERILJmZ4",
        "outputId": "57d63747-573a-4c7c-9497-75af907adfca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length = 14040  Characters [\"'\", ' ', ' ', ', ', '? ', ' ', ' ', '. ', ' ', ' ', ' ', ' ', '. ', ' ', \"'\", ' ', ' ', ' ', ' ', ' ', ', ', \"'\", '. ', '. ', '. ', \"'\", ' ', ' ', ' ', ' ', '. ', \"'\", ' ', ' ', ' ', ' ', \"'\", ' ', ' ', ' ', ' ', ' ', '. ', '. ', ' ', \"'\", ' ', '. ', ' ', ' ', ' ', ' ', '. ', ' ', '... ', ' ', ' ', ' ', ' ', '? ', '. ', \"'\", ' ', ' ', ' ', '. ', ' ', ', ', ' ', ' ', '. ', \"'\", ' ', ' ', ' ', ' ', \"'\", ' ', ' ', ' ', ' ', ' ', ' ', ' ', '. ', ', ', ' ', '. ', \"'\", ' ', '. ', ' ', \"'\", ' ', ' ', ' ', '. ', ' ', ' ', ' ', '. ', ' ', ' ', '? ', ' ', ' ', \"'\", ' ', ' ', '. ', ' ', ' ', \"'\", ' ', ' ', '. ', ', ', ' ', ' ', ' ', ' ', ' ', ' ', '? ', \"'\", ' ', ' ', ' ', '. ', ' ', '. ', ' ', ' ', ' ', ' ', ' ', ' ', '? ', '... ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', '. ', ' ', ' ', ' ', '. ', ' ', ' ', ', ', '. ', ' ', ' ', ' ', \"'\", ' ', '. ', ' ', ' ', \"'\", ' ', ' ', '. ', ' ', '? ', \"'\", ' ', ' ', ' ', ' ', '. ', ' ', ' ', ' ', ' ', '. ', \"'\", ' ', ' ', ' ', '. ', \"'\", ' ', '. ', ' ', ' ', ' ', '? ', ' ', '. ', ' ', ' ', ' ', '. ']\n",
            "Length = 14040  Characters ['It', 's', 'table', '6', 'right', 'I', 'think', 'so', 'Always', 'go', 'with', 'basic', 'assumptions', 'Unless', 'it', 's', 'about', 'someone', 'in', 'that', 'case', 'don', 't', 'Henry', 'Yes', 'I', 've', 'been', 'sitting', 'with', 'you', 'Don', 't', 'like', 'it', 'because', 'we', 're', 'right', 'in', 'front', 'of', 'Ms', 'Ronkel', 'But', 'it', 's', 'okay', 'You', 'can', 'live', 'with', 'it', 'Did', 'you', 'did', 'you', 'do', 'the', 'search', 'Yes', 'It', 's', 'for', 'next', 'class', 'I', 'know', 'I', 'found', 'out', 'I', 'm', 'so', 'mad', 'because', 'we', 're', 'not', 'going', 'to', 'be', 'here', 'next', 'class', 'Yeah', 'we', 'are', 'It', 's', 'okay', 'It', 'ain', 't', 'going', 'to', 'nothing', 'It', 'took', 'two', 'hours', 'Did', 'it', 'really', 'The', 'first', 'part', 's', '50', 'minutes', 'The', 'second', 'part', 's', '60', 'seconds', 'Wait', 'so', 'you', 'actually', 'have', 'to', 'read', 'it', 'It', 's', 'a', 'listening', 'test', 'Told', 'you', 'But', 'is', 'this', 'a', 'last', 'minute', 'test', 'Or', 'I', 'thought', 'it', 'was', 'going', 'to', 'take', 'like', '15', 'minutes', 'It', 'took', 'two', 'hours', 'Oh', 'my', 'god', 'okay', 'You', 'know', 'what', 'I', 'm', 'doing', 'I', 'think', 'you', 're', 'really', 'interesting', 'It', 'is', 'It', 's', 'about', 'South', 'Side', 'School', 'It', 'has', 'a', 'high', 'school', 'It', 's', 'a', 'math', 'class', 'It', 's', 'Athena', 'Is', 'it', 'VR', 'America', 'It', 'is', 'I', 'love', 'that', 'book']\n",
            "Length = 13119  Characters [\"It's\", 'table', '6,', 'right?', 'I', 'think', 'so.', 'Always', 'go', 'with', 'basic', 'assumptions.', 'Unless', \"it's\", 'about', 'someone', 'in', 'that', 'case,', \"don't.\", 'Henry.', 'Yes.', \"I've\", 'been', 'sitting', 'with', 'you.', \"Don't\", 'like', 'it', 'because', \"we're\", 'right', 'in', 'front', 'of', 'Ms.', 'Ronkel.', 'But', \"it's\", 'okay.', 'You', 'can', 'live', 'with', 'it.', 'Did', 'you...', 'did', 'you', 'do', 'the', 'search?', 'Yes.', \"It's\", 'for', 'next', 'class.', 'I', 'know,', 'I', 'found', 'out.', \"I'm\", 'so', 'mad', 'because', \"we're\", 'not', 'going', 'to', 'be', 'here', 'next', 'class.', 'Yeah,', 'we', 'are.', \"It's\", 'okay.', 'It', \"ain't\", 'going', 'to', 'nothing.', 'It', 'took', 'two', 'hours.', 'Did', 'it', 'really?', 'The', 'first', \"part's\", '50', 'minutes.', 'The', 'second', \"part's\", '60', 'seconds.', 'Wait,', 'so', 'you', 'actually', 'have', 'to', 'read', 'it?', \"It's\", 'a', 'listening', 'test.', 'Told', 'you.', 'But', 'is', 'this', 'a', 'last', 'minute', 'test?', 'Or...', 'I', 'thought', 'it', 'was', 'going', 'to', 'take', 'like', '15', 'minutes.', 'It', 'took', 'two', 'hours.', 'Oh', 'my', 'god,', 'okay.', 'You', 'know', 'what', \"I'm\", 'doing.', 'I', 'think', \"you're\", 'really', 'interesting.', 'It', 'is?', \"It's\", 'about', 'South', 'Side', 'School.', 'It', 'has', 'a', 'high', 'school.', \"It's\", 'a', 'math', 'class.', \"It's\", 'Athena.', 'Is', 'it', 'VR', 'America?', 'It', 'is.', 'I', 'love', 'that', 'book.', 'Oh', 'my', 'god.', 'Have', 'you', 'read', 'it?', 'Yeah.', \"It's\", 'crazy.', \"It's\", 'amazing.', 'You', 'have', 'to...', 'Are', 'you', 'taking', 'American', 'Lit?']\n",
            "Length = 13118  Characters [' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ']\n",
            "Length = 1849  Characters ['It', 'Always', 'Unless', 'Henry', 'Yes', 'Don', 'Ms', 'Ronkel', 'But', 'You', 'Did', 'Yes', 'It', 'Yeah', 'It', 'It', 'It', 'Did', 'The', 'The', 'Wait', 'It', 'Told', 'But', 'Or', 'It', 'Oh', 'You', 'It', 'It', 'South', 'Side', 'School', 'It', 'It', 'It', 'Athena', 'Is', 'VR', 'America', 'It', 'Oh', 'Have', 'Yeah', 'It', 'It', 'You', 'Are', 'American', 'Lit', 'You', 'The', 'Ms', 'Flanagan', 'Excellent', 'Yeah', 'It', 'It', 'You', 'Chicago', 'It', 'There', 'Thank', 'Thank', 'Thank', 'Did', 'We', 'So', 'What', 'By', 'Because', 'Oh', 'Whenever', 'Ms', 'Valesky', 'Ms', 'Valesky', 'Like', 'Ms', 'Park', 'Ms', 'Park', 'Ms', 'Park', 'She', 'Park', 'Why', 'Because', 'Dude', 'Yeah', 'Okay', 'How', 'It', 'Made', 'Melissa', 'She', 'Although', 'Now', 'Now', 'Why', 'It', 'You', 'Really', 'And', 'Here', 'Wait', 'So', 'It', 'Twenty', 'Positive', 'What', 'What', 'Twenty', 'It', 'It', 'Why', 'Oh', 'Twenty', 'Yeah', 'Period', 'So', 'Yeah', 'But', 'No', 'As', 'Like', 'Whereas', 'Which', 'Yeah', 'What', 'The', 'Oh', 'No', 'Oh', 'Me', 'Oh', 'Oh', 'Fifteen', 'Wait', 'That', 'They', 'Right', 'No', 'No', 'Do', 'It', 'Think', 'If', 'Then', 'No', 'If', 'They', 'Oh', 'Wait', 'Because', 'This', 'Okay', 'So', 'Why', 'Anyone', 'Yeah', 'Or', 'Tim', 'Oh', 'God', 'If', 'First', 'By', 'Andrew', 'Shit', 'Is', 'Thursday', 'It', 'Wednesday', 'Thursday', 'No', 'PSAE', 'Wednesday', 'PSAE', 'Thursday', 'Wednesday', 'PSAE', 'PSAE', 'We', 'Thursday', 'We', 'Yeah', 'We', 'Yeah', 'And', 'No', 'They', 'Facebook', 'Like', 'Mr', 'Finnelli', 'He', 'Mr', 'Finnelli', 'Mr']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Regular expression tokenizing Gaps =False\n",
        "pattern = r'\\w+'\n",
        "text_regexTokensWT = nltk.regexp_tokenize(text_tokens, pattern, gaps=True)\n",
        "\n",
        "#Regex Gaps=True\n",
        "pattern = r'\\w+'\n",
        "text_regexTokensWF = nltk.regexp_tokenize(text_tokens, pattern, gaps=False)\n",
        "\n",
        "# Regular expression tokenizing Gaps =False\n",
        "pattern = r'\\s+'\n",
        "text_regexTokensST = nltk.regexp_tokenize(text_tokens, pattern, gaps=True)\n",
        "\n",
        "#Regex Gaps=True\n",
        "pattern = r'\\s+'\n",
        "text_regexTokensSF = nltk.regexp_tokenize(text_tokens, pattern, gaps=False)\n",
        "\n",
        "#Words with capital letters\n",
        "pattern3 = r'[A-Z]\\w+'\n",
        "text_regexTokensCAPS = nltk.regexp_tokenize(text_tokens, pattern3)\n",
        "\n",
        "print(\"Length =\", len(text_regexTokensWT),\" Characters\", text_regexTokensWT[0:200])\n",
        "print(\"Length =\", len(text_regexTokensWF),\" Characters\", text_regexTokensWF[0:200])\n",
        "print(\"Length =\", len(text_regexTokensST),\" Characters\", text_regexTokensST[0:200])\n",
        "print(\"Length =\", len(text_regexTokensSF),\" Characters\", text_regexTokensSF[0:200])\n",
        "print(\"Length =\", len(text_regexTokensCAPS),\" Characters\", text_regexTokensCAPS[0:200])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the list to a set to get unique words, then sort it\n",
        "unique_sorted_words = sorted(set(text_regexTokensCAPS))\n",
        "\n",
        "# Save the sorted unique words to a text file in your Google Drive, with each word on a new line\n",
        "with open('/content/drive/MyDrive/text_regexTokensCAPS.txt', 'w') as file:\n",
        "    for word in unique_sorted_words:\n",
        "        file.write(\"%s\\n\" % word)\n",
        "\n",
        "print(\"Text saved to text_regexTokensCAPS.txt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RR9vch6Zw6_v",
        "outputId": "ce197660-21de-488b-ad7a-3694b6505c52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text saved to text_regexTokensCAPS.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pTEr6oDShgc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "888d1fde-5fce-4b0d-b05a-971194d54a4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'str'>\n",
            "<class 'str'>\n",
            "<class 'list'>\n",
            "<class 'list'>\n",
            "<class 'list'>\n",
            "<class 'list'>\n",
            "<class 'nltk.text.Text'>\n"
          ]
        }
      ],
      "source": [
        "#What types of variables are these\n",
        "print(type(raw_uncertaintyText))\n",
        "print(type(text_tokens))\n",
        "print(type(text_wordTokens))\n",
        "print(type(text_sentTokens))\n",
        "print(type(text_regexTokensST))\n",
        "print(type(text_regexTokensSF))\n",
        "print(type(text_Objects))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xu9LrUG8CIjT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "242da06b-22d2-4eb6-b7c0-d3ef6e9e834b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "66216\n",
            "66216\n",
            "16770\n",
            "2023\n",
            "13119\n",
            "16770\n"
          ]
        }
      ],
      "source": [
        "print(len(raw_uncertaintyText))\n",
        "print(len(text_tokens))\n",
        "print(len(text_wordTokens))\n",
        "print(len(text_sentTokens))\n",
        "print(len(text_regexTokensST))\n",
        "print(len(text_Objects))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hn0XQpMTAxRn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90a7f58c-18d7-4b3b-cf3f-96b9fd59d12e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Always go with basic assumptions.',\n",
              " 'But is this a last minute test?',\n",
              " \"But it's okay.\",\n",
              " 'Did it really?',\n",
              " 'Did you... did you do the search?',\n",
              " \"Don't like it because we're right in front of Ms. Ronkel.\",\n",
              " 'Excellent.',\n",
              " 'Have you read it?',\n",
              " 'Henry.',\n",
              " 'I know, I found out.',\n",
              " 'I love that book.',\n",
              " 'I mean, no.',\n",
              " 'I think so.',\n",
              " \"I think you're really interesting.\",\n",
              " 'I thought it was going to take like 15 minutes.',\n",
              " \"I'm so mad because we're not going to be here next class.\",\n",
              " \"I've been sitting with you.\",\n",
              " 'Is it VR America?',\n",
              " \"It ain't going to nothing.\",\n",
              " 'It has a high school.',\n",
              " 'It is.',\n",
              " 'It is?',\n",
              " 'It took two hours.',\n",
              " \"It's Athena.\",\n",
              " \"It's a listening test.\",\n",
              " \"It's a math class.\",\n",
              " \"It's about South Side School.\",\n",
              " \"It's amazing.\",\n",
              " \"It's crazy.\",\n",
              " \"It's for next class.\",\n",
              " \"It's okay.\",\n",
              " \"It's table 6, right?\",\n",
              " 'Oh my god, okay.',\n",
              " 'Oh my god.',\n",
              " 'Or...',\n",
              " \"The first part's 50 minutes.\",\n",
              " 'The other class was Ms. Flanagan.',\n",
              " \"The second part's 60 seconds.\",\n",
              " 'Told you.',\n",
              " \"Unless it's about someone in that case, don't.\",\n",
              " 'Wait, so you actually have to read it?',\n",
              " 'Yeah, we are.',\n",
              " 'Yeah.',\n",
              " 'Yes.',\n",
              " 'You can live with it.',\n",
              " 'You have to... Are you taking American Lit?',\n",
              " \"You know what I'm doing.\",\n",
              " 'You took that already.']"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "#sorted(set(raw_uncertaintyText[0:50]))\n",
        "len(text_tokens)\n",
        "sorted(set(text_wordTokens[0:50]))\n",
        "sorted(set(text_sentTokens[0:50]))\n",
        "#sorted(set(text_Objects[0:50]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VJ8NdutmkU2"
      },
      "source": [
        "Extract only nouns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BnPpypbgmeZ8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "2a90f083-178f-434b-8f32-0263a8a8d823"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-e8e1427ec387>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtext_regexTokensST_only_nn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'NN'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFreqDist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_regexTokensST_only_nn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pos' is not defined"
          ]
        }
      ],
      "source": [
        "text_regexTokensST_only_nn = [x for (x,y) in pos if y in ('NN')]\n",
        "\n",
        "freq = nltk.FreqDist(text_regexTokensST_only_nn)\n",
        "freq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZZACFNXfuet"
      },
      "outputs": [],
      "source": [
        "#Combined All in one normalizer\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# split into word tokens\n",
        "#from nltk.tokenize import word_tokenize\n",
        "#word_tokens = word_tokenize(text_file)\n",
        "\n",
        "# convert to lower case\n",
        "lower_tokens = [w.lower() for w in text_regexTokensST]\n",
        "\n",
        "# remove punctuation from each word\n",
        "import string\n",
        "table = str.maketrans('', '', string.punctuation)\n",
        "stripped = [w.translate(table) for w in lower_tokens]\n",
        "\n",
        "\n",
        "# remove remaining tokens that are not alphabetic\n",
        "alpha_words = [word for word in stripped if word.isalpha()]\n",
        "\n",
        "\n",
        "# filter out stop words\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "code4 = [w for w in alpha_words if not w in stop_words]\n",
        "print(code4[:100])\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}